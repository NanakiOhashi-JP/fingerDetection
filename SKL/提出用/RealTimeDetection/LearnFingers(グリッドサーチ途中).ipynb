{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "uih91uiwI5Uj"
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "# !pip install opencv-python scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zpwvMvkEI_as"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import glob\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ychLA0c1TqgN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#--------------------------------------------------\n",
    "# 【提出に関して】kaggleのAPIキーを入れるのは無理なので、\n",
    "# 一時的に実行可能な形にリファクタリングします。\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Kaggleデータセットのダウンロード（初回のみ）\n",
    "def download_kaggle_dataset():\n",
    "       print(\"Kaggleデータセットをダウンロード中...\")\n",
    "\n",
    "    # Kaggle APIの設定\n",
    "    !pip install kaggle\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "    # データセットをダウンロード\n",
    "    !kaggle datasets download -d koryakinp/fingers\n",
    "\n",
    "    # ZIPファイルを解凍\n",
    "    !unzip -q fingers.zip -d /content/kaggle_fingers/\n",
    "\n",
    "    print(\"ダウンロード完了\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gJx3-qP-KJmF"
   },
   "outputs": [],
   "source": [
    "def load_image_resized(image_path, target_size=(64, 64)):\n",
    "    \"\"\"画像を読み込んでリサイズ\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, target_size)\n",
    "    return resized.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFeiFaakKL7P"
   },
   "outputs": [],
   "source": [
    "def load_your_dataset(base_folder):\n",
    "    \"\"\"データセットを読み込み\"\"\"\n",
    "    print(\"データセットを読み込み中...\")\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    if not os.path.exists(base_folder):\n",
    "        print(f\"警告: {base_folder} が見つかりません\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    subfolders = [d for d in os.listdir(base_folder)\n",
    "                  if os.path.isdir(os.path.join(base_folder, d))]\n",
    "\n",
    "    for folder_name in subfolders:\n",
    "        finger_count = int(folder_name.split('-')[0])\n",
    "        folder_path = os.path.join(base_folder, folder_name)\n",
    "\n",
    "        image_files = glob.glob(os.path.join(folder_path, \"*.jpg\")) + \\\n",
    "                     glob.glob(os.path.join(folder_path, \"*.png\"))\n",
    "\n",
    "        print(f\"フォルダ {folder_name} (指の本数: {finger_count}) - 画像数: {len(image_files)}\")\n",
    "\n",
    "        for image_path in image_files:\n",
    "            image_data = load_image_resized(image_path)\n",
    "            if image_data is not None:\n",
    "                X.append(image_data)\n",
    "                y.append(finger_count)\n",
    "\n",
    "    print(f\"データセット読み込み完了: {len(X)}枚\")\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUTb0l4EKZ5S"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def load_kaggle_dataset(kaggle_folder=\"/content/kaggle_fingers\"):\n",
    "    \"\"\"Kaggleデータセットを読み込み（ファイル名からラベル抽出）\"\"\"\n",
    "    print(\"Kaggleデータセットを読み込み中...\")\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    target_folders = [\"train\", \"fingers\"]  # 利用対象\n",
    "\n",
    "    for folder_name in target_folders:\n",
    "        folder_path = os.path.join(kaggle_folder, folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"警告: {folder_path} が存在しません\")\n",
    "            continue\n",
    "\n",
    "        image_files = glob.glob(os.path.join(folder_path, \"*.png\")) + \\\n",
    "                      glob.glob(os.path.join(folder_path, \"*.jpg\")) + \\\n",
    "                      glob.glob(os.path.join(folder_path, \"*.jpeg\"))\n",
    "\n",
    "        print(f\"{folder_name}/: 画像数: {len(image_files)}\")\n",
    "\n",
    "        for image_path in image_files:\n",
    "            filename = os.path.basename(image_path)\n",
    "            match = re.search(r\"_([0-5])[RL]\\.\", filename)\n",
    "            if match:\n",
    "                finger_count = int(match.group(1))\n",
    "                image_data = load_image_resized(image_path)\n",
    "                if image_data is not None:\n",
    "                    X.append(image_data)\n",
    "                    y.append(finger_count)\n",
    "            else:\n",
    "                print(f\"ラベル抽出失敗: {filename}\")\n",
    "\n",
    "    print(f\"Kaggleデータセット読み込み完了: {len(X)}枚\")\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cz9ePH1IKart"
   },
   "outputs": [],
   "source": [
    "def combine_datasets(X1, y1, X2, y2):\n",
    "    \"\"\"2つのデータセットを結合\"\"\"\n",
    "    if len(X1) == 0:\n",
    "        return X2, y2\n",
    "    if len(X2) == 0:\n",
    "        return X1, y1\n",
    "\n",
    "    X_combined = np.vstack([X1, X2])\n",
    "    y_combined = np.hstack([y1, y2])\n",
    "\n",
    "    print(f\"結合後のデータセット: {len(X_combined)}枚\")\n",
    "    print(f\"クラス別データ数:\")\n",
    "    for finger_count in sorted(set(y_combined)):\n",
    "        count = np.sum(y_combined == finger_count)\n",
    "        print(f\"  {finger_count}本指: {count}枚\")\n",
    "\n",
    "    return X_combined, y_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZHohX8XKgBc"
   },
   "outputs": [],
   "source": [
    "def train_combined_model():\n",
    "    \"\"\"結合データセットでモデルを訓練\"\"\"\n",
    "\n",
    "    # 1. あなたのデータセットを読み込み\n",
    "    # ---------------------------------\n",
    "    # 実際はGoogle Driveをマウントして直接パスを通してます。\n",
    "    # ---------------------------------\n",
    "    your_data_folder = \"/content/frames\"\n",
    "    X_yours, y_yours = load_your_dataset(your_data_folder)\n",
    "\n",
    "    # 2. Kaggleデータセットを読み込み\n",
    "    X_kaggle, y_kaggle = load_kaggle_dataset()\n",
    "\n",
    "    # 3. データセットを結合\n",
    "    X_combined, y_combined = combine_datasets(X_yours, y_yours, X_kaggle, y_kaggle)\n",
    "\n",
    "    if len(X_combined) == 0:\n",
    "        print(\"エラー: データが読み込めませんでした\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nモデル訓練開始\")\n",
    "    print(f\"総データ数: {len(X_combined)}枚\")\n",
    "\n",
    "    # データ分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n",
    "    )\n",
    "\n",
    "    print(f\"訓練用: {len(X_train)}枚, テスト用: {len(X_test)}枚\")\n",
    "\n",
    "    # モデル訓練（より大きなデータセットに対応）\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200,  # 木の数を増加\n",
    "        max_depth=15,      # 深さを増加\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(\"モデル訓練中...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 評価\n",
    "    print(\"\\nモデル評価\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"テストセット精度: {accuracy:.3f}\")\n",
    "    print(\"\\n詳細な分類レポート:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # あなたのデータだけでの性能も確認\n",
    "    if len(X_yours) > 0:\n",
    "        print(\"\\nあなたのデータのみでの性能\")\n",
    "        your_indices = np.arange(len(X_yours))\n",
    "        if len(your_indices) > 10:  # 十分なデータがある場合\n",
    "            X_your_train, X_your_test, y_your_train, y_your_test = train_test_split(\n",
    "                X_yours, y_yours, test_size=0.3, random_state=42\n",
    "            )\n",
    "            y_your_pred = model.predict(X_your_test)\n",
    "            your_accuracy = accuracy_score(y_your_test, y_your_pred)\n",
    "            print(f\"あなたのデータでの精度: {your_accuracy:.3f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjdR4hnNKmqO"
   },
   "outputs": [],
   "source": [
    "def save_combined_model(model):\n",
    "    \"\"\"結合学習したモデルを保存します\"\"\"\n",
    "    if model is None:\n",
    "        return\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    model_path = \"/content/finger_model_combined.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    print(f\"\\n結合学習モデルを保存しました: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def hyperparameter_tuning(X_combined, y_combined):\n",
    "    \"\"\"\n",
    "    過学習を防ぐためのハイパーパラメータチューニング\n",
    "    \"\"\"\n",
    "    print(\"=== ハイパーパラメータチューニング開始 ===\")\n",
    "    \n",
    "    # パラメータグリッドを定義（過学習を抑制するため）\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],  # 木の数を減らして過学習を抑制\n",
    "        'max_depth': [5, 10, 15],        # 深さを制限\n",
    "        'min_samples_split': [10, 20, 50], # 分割に必要な最小サンプル数を増加\n",
    "        'min_samples_leaf': [5, 10, 20],   # 葉ノードの最小サンプル数を増加\n",
    "        'max_features': ['sqrt', 'log2']   # 特徴量の選択数を制限\n",
    "    }\n",
    "    \n",
    "    # ベースモデル\n",
    "    rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # グリッドサーチ（3-fold クロスバリデーション）\n",
    "    grid_search = GridSearchCV(\n",
    "        rf_base, \n",
    "        param_grid, \n",
    "        cv=3,  # データが少ない場合は3-fold\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"グリッドサーチ実行中...\")\n",
    "    grid_search.fit(X_combined, y_combined)\n",
    "    \n",
    "    print(f\"最適パラメータ: {grid_search.best_params_}\")\n",
    "    print(f\"最高CVスコア: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def evaluate_model_with_cv(model, X, y):\n",
    "    \"\"\"\n",
    "    クロスバリデーションでモデルを評価\n",
    "    \"\"\"\n",
    "    print(\"\\n=== クロスバリデーション評価 ===\")\n",
    "    \n",
    "    # 5-fold クロスバリデーション\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print(f\"CV平均精度: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    print(f\"各Foldの精度: {cv_scores}\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "def plot_confusion_matrix(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    混同行列を可視化\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(6), yticklabels=range(6))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n詳細な分類レポート:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, top_n=20):\n",
    "    \"\"\"\n",
    "    特徴量の重要度を可視化\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:top_n]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f'Top {top_n} Feature Importances')\n",
    "        plt.bar(range(top_n), importances[indices])\n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Top {top_n} 重要な特徴量のインデックス: {indices}\")\n",
    "\n",
    "def analyze_class_distribution(y):\n",
    "    \"\"\"\n",
    "    クラス分布を分析・可視化\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(unique, counts)\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Number of Fingers')\n",
    "    plt.ylabel('Count')\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(unique[i], count + 10, str(count), ha='center')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"クラス分布:\")\n",
    "    for finger_count, count in zip(unique, counts):\n",
    "        percentage = count / len(y) * 100\n",
    "        print(f\"  {finger_count}本指: {count}枚 ({percentage:.1f}%)\")\n",
    "    \n",
    "    # クラス不均衡のチェック\n",
    "    min_count, max_count = min(counts), max(counts)\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    print(f\"\\nクラス不均衡比: {imbalance_ratio:.2f}\")\n",
    "    if imbalance_ratio > 3:\n",
    "        print(\"警告: クラス不均衡が大きいです。層化サンプリングやバランシング手法を検討してください。\")\n",
    "\n",
    "def save_model_with_metadata(model, accuracy, cv_scores, filename=\"finger_model_optimized.pkl\"):\n",
    "    \"\"\"\n",
    "    モデルとメタデータを保存\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    \n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'test_accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'cv_scores': cv_scores,\n",
    "        'params': model.get_params(),\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'feature_size': model.n_features_in_ if hasattr(model, 'n_features_in_') else 'unknown'\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    print(f\"\\n最適化されたモデルとメタデータを保存: {filename}\")\n",
    "    print(f\"テスト精度: {accuracy:.3f}\")\n",
    "    print(f\"CV平均精度: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rdo4Lyp-XPcl",
    "outputId": "e857f487-a755-4de9-d22a-83b3b6410d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggleデータセットをダウンロード中...\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
      "Dataset URL: https://www.kaggle.com/datasets/koryakinp/fingers\n",
      "License(s): CC0-1.0\n",
      "fingers.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "replace /content/kaggle_fingers/fingers/test/000e7aa6-100b-4c6b-9ff0-e7a8e53e4465_5L.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
      "ダウンロード完了\n"
     ]
    }
   ],
   "source": [
    "# download_kaggle_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3k0QO0QQXwt",
    "outputId": "346b6a48-2394-4e1d-a868-9d3d153c0e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== データセットを読み込み中 ===\n",
      "フォルダ 0-1 (1) (指の本数: 0) - 画像数: 270\n",
      "フォルダ 2-1 (指の本数: 2) - 画像数: 213\n",
      "フォルダ 1-1 (指の本数: 1) - 画像数: 306\n",
      "フォルダ 4-1 (指の本数: 4) - 画像数: 198\n",
      "フォルダ 3-1 (指の本数: 3) - 画像数: 388\n",
      "フォルダ 5-1 (指の本数: 5) - 画像数: 276\n",
      "フォルダ 1-2 (指の本数: 1) - 画像数: 632\n",
      "フォルダ 2-2 (指の本数: 2) - 画像数: 785\n",
      "フォルダ 0-2 (指の本数: 0) - 画像数: 579\n",
      "フォルダ 5-2 (指の本数: 5) - 画像数: 660\n",
      "フォルダ 4-2 (指の本数: 4) - 画像数: 606\n",
      "フォルダ 3-2 (指の本数: 3) - 画像数: 847\n",
      "あなたのデータセット: 5760枚読み込み完了\n",
      "=== Kaggleデータセットを読み込み中 ===\n",
      "train/: 画像数: 18000\n",
      "fingers/: 画像数: 0\n",
      "Kaggleデータセット: 18000枚読み込み完了\n",
      "結合後のデータセット: 23760枚\n",
      "クラス別データ数:\n",
      "  0本指: 3849枚\n",
      "  1本指: 3938枚\n",
      "  2本指: 3998枚\n",
      "  3本指: 4235枚\n",
      "  4本指: 3804枚\n",
      "  5本指: 3936枚\n",
      "\n",
      "=== モデル訓練開始 ===\n",
      "総データ数: 23760枚\n",
      "訓練用: 19008枚, テスト用: 4752枚\n",
      "モデル訓練中...\n",
      "\n",
      "=== モデル評価 ===\n",
      "テストセット精度: 1.000\n",
      "\n",
      "詳細な分類レポート:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       770\n",
      "           1       1.00      1.00      1.00       787\n",
      "           2       1.00      1.00      1.00       800\n",
      "           3       1.00      1.00      1.00       847\n",
      "           4       1.00      1.00      1.00       761\n",
      "           5       1.00      1.00      1.00       787\n",
      "\n",
      "    accuracy                           1.00      4752\n",
      "   macro avg       1.00      1.00      1.00      4752\n",
      "weighted avg       1.00      1.00      1.00      4752\n",
      "\n",
      "\n",
      "=== あなたのデータのみでの性能 ===\n",
      "あなたのデータでの精度: 1.000\n",
      "\n",
      "結合学習モデルを保存しました: /content/drive/MyDrive/finger_model_combined_2.pkl\n",
      "このファイルをダウンロードしてWebカメラプログラムで使用してください\n",
      "\n",
      "=== 完了 ===\n"
     ]
    }
   ],
   "source": [
    "# メイン実行\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== 過学習対策付き指認識モデル訓練 ===\")\n",
    "    \n",
    "    # データ分布の事前分析\n",
    "    print(\"\\n1. データセット読み込みと分析...\")\n",
    "    your_data_folder = \"/content/frames\"\n",
    "    X_yours, y_yours = load_your_dataset(your_data_folder)\n",
    "    X_kaggle, y_kaggle = load_kaggle_dataset()\n",
    "    X_combined, y_combined = combine_datasets(X_yours, y_yours, X_kaggle, y_kaggle)\n",
    "    \n",
    "    if len(X_combined) > 0:\n",
    "        print(\"\\n2. クラス分布分析...\")\n",
    "        analyze_class_distribution(y_combined)\n",
    "    \n",
    "    # モデル訓練（ハイパーパラメータチューニング付き）\n",
    "    print(\"\\n3. モデル訓練開始...\")\n",
    "    optimized_model = train_combined_model()\n",
    "    \n",
    "    if optimized_model is not None:\n",
    "        # 特徴量重要度の分析\n",
    "        print(\"\\n4. 特徴量重要度分析...\")\n",
    "        plot_feature_importance(optimized_model)\n",
    "        \n",
    "        # モデル保存（メタデータ付き）\n",
    "        print(\"\\n5. モデル保存...\")\n",
    "        # テスト精度とCVスコアを取得（既に計算済み）\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n",
    "        )\n",
    "        test_accuracy = optimized_model.score(X_test, y_test)\n",
    "        cv_scores = cross_val_score(optimized_model, X_train, y_train, cv=5)\n",
    "        \n",
    "        save_model_with_metadata(optimized_model, test_accuracy, cv_scores)\n",
    "        \n",
    "        print(\"\\n=== 完了 ===\")\n",
    "        print(\"過学習を抑制した最適化モデルが作成されました。\")\n",
    "    else:\n",
    "        print(\"エラー: モデル訓練に失敗しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nz6ehfwhUQUQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
